Progress[  0%], ETA[  19m], Batch [  10], G_Loss[0.281], D_Real_Loss[1.694], D_Fake_Loss[0.339]
Progress[  0%], ETA[  19m], Batch [  20], G_Loss[0.193], D_Real_Loss[1.140], D_Fake_Loss[0.453]
Progress[  1%], ETA[  19m], Batch [  30], G_Loss[0.156], D_Real_Loss[0.837], D_Fake_Loss[0.542]
Progress[  1%], ETA[  19m], Batch [  40], G_Loss[0.146], D_Real_Loss[0.744], D_Fake_Loss[0.769]
Progress[  1%], ETA[  19m], Batch [  50], G_Loss[0.139], D_Real_Loss[0.620], D_Fake_Loss[0.811]
Progress[  2%], ETA[  19m], Batch [  60], G_Loss[0.127], D_Real_Loss[0.646], D_Fake_Loss[0.810]
Progress[  2%], ETA[  19m], Batch [  70], G_Loss[0.150], D_Real_Loss[0.881], D_Fake_Loss[0.607]
Progress[  2%], ETA[  19m], Batch [  80], G_Loss[0.121], D_Real_Loss[0.653], D_Fake_Loss[0.700]
Progress[  3%], ETA[  19m], Batch [  90], G_Loss[0.149], D_Real_Loss[0.791], D_Fake_Loss[0.851]
Progress[  3%], ETA[  19m], Batch [ 100], G_Loss[0.130], D_Real_Loss[0.685], D_Fake_Loss[0.672]
Progress[  3%], ETA[  19m], Batch [ 110], G_Loss[0.118], D_Real_Loss[0.733], D_Fake_Loss[0.648]
Progress[  4%], ETA[  19m], Batch [ 120], G_Loss[0.114], D_Real_Loss[0.709], D_Fake_Loss[0.673]
Progress[  4%], ETA[  19m], Batch [ 130], G_Loss[0.132], D_Real_Loss[0.705], D_Fake_Loss[0.661]
Progress[  4%], ETA[  19m], Batch [ 140], G_Loss[0.138], D_Real_Loss[0.885], D_Fake_Loss[0.548]
Progress[  5%], ETA[  18m], Batch [ 150], G_Loss[0.123], D_Real_Loss[0.801], D_Fake_Loss[0.642]
Progress[  5%], ETA[  18m], Batch [ 160], G_Loss[0.144], D_Real_Loss[0.824], D_Fake_Loss[0.621]
Progress[  5%], ETA[  18m], Batch [ 170], G_Loss[0.122], D_Real_Loss[0.708], D_Fake_Loss[0.692]
Progress[  6%], ETA[  18m], Batch [ 180], G_Loss[0.123], D_Real_Loss[0.781], D_Fake_Loss[0.652]
Progress[  6%], ETA[  18m], Batch [ 190], G_Loss[0.113], D_Real_Loss[0.604], D_Fake_Loss[0.761]
Progress[  6%], ETA[  18m], Batch [ 200], G_Loss[0.123], D_Real_Loss[0.669], D_Fake_Loss[0.685]
    Saved train/batch000200_out.png
Progress[  7%], ETA[  18m], Batch [ 210], G_Loss[0.105], D_Real_Loss[0.619], D_Fake_Loss[0.823]
Progress[  7%], ETA[  18m], Batch [ 220], G_Loss[0.109], D_Real_Loss[0.622], D_Fake_Loss[0.734]
Progress[  7%], ETA[  18m], Batch [ 230], G_Loss[0.132], D_Real_Loss[0.746], D_Fake_Loss[0.484]
Progress[  8%], ETA[  18m], Batch [ 240], G_Loss[0.125], D_Real_Loss[0.730], D_Fake_Loss[0.597]
Progress[  8%], ETA[  18m], Batch [ 250], G_Loss[0.107], D_Real_Loss[0.579], D_Fake_Loss[0.830]
Progress[  8%], ETA[  18m], Batch [ 260], G_Loss[0.109], D_Real_Loss[0.594], D_Fake_Loss[0.690]
Progress[  9%], ETA[  18m], Batch [ 270], G_Loss[0.129], D_Real_Loss[0.754], D_Fake_Loss[0.531]
Progress[  9%], ETA[  18m], Batch [ 280], G_Loss[0.098], D_Real_Loss[0.596], D_Fake_Loss[0.747]
Progress[  9%], ETA[  18m], Batch [ 290], G_Loss[0.134], D_Real_Loss[0.683], D_Fake_Loss[0.657]
Progress[ 10%], ETA[  17m], Batch [ 300], G_Loss[0.126], D_Real_Loss[0.705], D_Fake_Loss[0.658]
Progress[ 10%], ETA[  17m], Batch [ 310], G_Loss[0.127], D_Real_Loss[0.862], D_Fake_Loss[0.542]
Progress[ 10%], ETA[  17m], Batch [ 320], G_Loss[0.156], D_Real_Loss[0.970], D_Fake_Loss[0.545]
Progress[ 11%], ETA[  17m], Batch [ 330], G_Loss[0.147], D_Real_Loss[0.958], D_Fake_Loss[0.594]
Progress[ 11%], ETA[  17m], Batch [ 340], G_Loss[0.120], D_Real_Loss[0.592], D_Fake_Loss[0.744]
Progress[ 11%], ETA[  17m], Batch [ 350], G_Loss[0.136], D_Real_Loss[0.594], D_Fake_Loss[0.679]
Progress[ 12%], ETA[  17m], Batch [ 360], G_Loss[0.123], D_Real_Loss[0.748], D_Fake_Loss[0.563]
Progress[ 12%], ETA[  17m], Batch [ 370], G_Loss[0.122], D_Real_Loss[0.686], D_Fake_Loss[0.623]
Progress[ 12%], ETA[  17m], Batch [ 380], G_Loss[0.112], D_Real_Loss[0.648], D_Fake_Loss[0.691]
Progress[ 13%], ETA[  17m], Batch [ 390], G_Loss[0.118], D_Real_Loss[0.734], D_Fake_Loss[0.570]
Progress[ 13%], ETA[  17m], Batch [ 400], G_Loss[0.118], D_Real_Loss[0.641], D_Fake_Loss[0.643]
    Saved train/batch000400_out.png
Progress[ 13%], ETA[  17m], Batch [ 410], G_Loss[0.115], D_Real_Loss[0.672], D_Fake_Loss[0.681]
Progress[ 14%], ETA[  17m], Batch [ 420], G_Loss[0.108], D_Real_Loss[0.520], D_Fake_Loss[0.740]
Progress[ 14%], ETA[  17m], Batch [ 430], G_Loss[0.127], D_Real_Loss[0.691], D_Fake_Loss[0.628]
Progress[ 14%], ETA[  17m], Batch [ 440], G_Loss[0.121], D_Real_Loss[0.653], D_Fake_Loss[0.556]
Progress[ 15%], ETA[  16m], Batch [ 450], G_Loss[0.108], D_Real_Loss[0.680], D_Fake_Loss[0.644]
Progress[ 15%], ETA[  16m], Batch [ 460], G_Loss[0.111], D_Real_Loss[0.502], D_Fake_Loss[0.721]
Progress[ 15%], ETA[  16m], Batch [ 470], G_Loss[0.123], D_Real_Loss[0.779], D_Fake_Loss[0.639]
Progress[ 16%], ETA[  16m], Batch [ 480], G_Loss[0.111], D_Real_Loss[0.593], D_Fake_Loss[0.727]
Progress[ 16%], ETA[  16m], Batch [ 490], G_Loss[0.113], D_Real_Loss[0.690], D_Fake_Loss[0.630]
Progress[ 16%], ETA[  16m], Batch [ 500], G_Loss[0.139], D_Real_Loss[0.737], D_Fake_Loss[0.673]
Progress[ 17%], ETA[  16m], Batch [ 510], G_Loss[0.141], D_Real_Loss[0.829], D_Fake_Loss[0.476]
Progress[ 17%], ETA[  16m], Batch [ 520], G_Loss[0.116], D_Real_Loss[0.722], D_Fake_Loss[0.664]
Progress[ 17%], ETA[  16m], Batch [ 530], G_Loss[0.107], D_Real_Loss[0.619], D_Fake_Loss[0.773]
Progress[ 18%], ETA[  16m], Batch [ 540], G_Loss[0.112], D_Real_Loss[0.527], D_Fake_Loss[0.719]
Progress[ 18%], ETA[  16m], Batch [ 550], G_Loss[0.125], D_Real_Loss[0.631], D_Fake_Loss[0.530]
Progress[ 18%], ETA[  16m], Batch [ 560], G_Loss[0.128], D_Real_Loss[0.512], D_Fake_Loss[0.630]
Progress[ 19%], ETA[  16m], Batch [ 570], G_Loss[0.108], D_Real_Loss[0.532], D_Fake_Loss[0.724]
Progress[ 19%], ETA[  16m], Batch [ 580], G_Loss[0.121], D_Real_Loss[0.537], D_Fake_Loss[0.730]
Progress[ 19%], ETA[  16m], Batch [ 590], G_Loss[0.126], D_Real_Loss[0.630], D_Fake_Loss[0.629]
Progress[ 20%], ETA[  15m], Batch [ 600], G_Loss[0.118], D_Real_Loss[0.668], D_Fake_Loss[0.611]
    Saved train/batch000600_out.png
Progress[ 20%], ETA[  15m], Batch [ 610], G_Loss[0.099], D_Real_Loss[0.516], D_Fake_Loss[0.794]
Progress[ 20%], ETA[  15m], Batch [ 620], G_Loss[0.152], D_Real_Loss[0.826], D_Fake_Loss[0.676]
Progress[ 21%], ETA[  15m], Batch [ 630], G_Loss[0.149], D_Real_Loss[0.560], D_Fake_Loss[0.464]
Progress[ 21%], ETA[  15m], Batch [ 640], G_Loss[0.098], D_Real_Loss[0.556], D_Fake_Loss[0.800]
Progress[ 21%], ETA[  15m], Batch [ 650], G_Loss[0.110], D_Real_Loss[0.606], D_Fake_Loss[0.688]
Progress[ 22%], ETA[  15m], Batch [ 660], G_Loss[0.155], D_Real_Loss[0.960], D_Fake_Loss[0.402]
Progress[ 22%], ETA[  15m], Batch [ 670], G_Loss[0.125], D_Real_Loss[0.844], D_Fake_Loss[0.553]
Progress[ 22%], ETA[  15m], Batch [ 680], G_Loss[0.153], D_Real_Loss[0.712], D_Fake_Loss[0.499]
Progress[ 23%], ETA[  15m], Batch [ 690], G_Loss[0.130], D_Real_Loss[0.525], D_Fake_Loss[0.683]
Progress[ 23%], ETA[  15m], Batch [ 700], G_Loss[0.125], D_Real_Loss[0.616], D_Fake_Loss[0.634]
Progress[ 23%], ETA[  15m], Batch [ 710], G_Loss[0.149], D_Real_Loss[0.614], D_Fake_Loss[0.761]
Progress[ 24%], ETA[  15m], Batch [ 720], G_Loss[0.135], D_Real_Loss[0.512], D_Fake_Loss[0.562]
Progress[ 24%], ETA[  15m], Batch [ 730], G_Loss[0.116], D_Real_Loss[0.547], D_Fake_Loss[0.618]
Progress[ 24%], ETA[  15m], Batch [ 740], G_Loss[0.136], D_Real_Loss[0.711], D_Fake_Loss[0.503]
Progress[ 25%], ETA[  14m], Batch [ 750], G_Loss[0.112], D_Real_Loss[0.461], D_Fake_Loss[0.749]
Progress[ 25%], ETA[  14m], Batch [ 760], G_Loss[0.098], D_Real_Loss[0.470], D_Fake_Loss[0.845]
Progress[ 25%], ETA[  14m], Batch [ 770], G_Loss[0.159], D_Real_Loss[0.722], D_Fake_Loss[0.374]
Progress[ 26%], ETA[  14m], Batch [ 780], G_Loss[0.134], D_Real_Loss[0.757], D_Fake_Loss[0.580]
Progress[ 26%], ETA[  14m], Batch [ 790], G_Loss[0.155], D_Real_Loss[0.652], D_Fake_Loss[0.415]
Progress[ 26%], ETA[  14m], Batch [ 800], G_Loss[0.135], D_Real_Loss[0.700], D_Fake_Loss[0.504]
    Saved train/batch000800_out.png
Progress[ 27%], ETA[  14m], Batch [ 810], G_Loss[0.123], D_Real_Loss[0.427], D_Fake_Loss[0.737]
Progress[ 27%], ETA[  14m], Batch [ 820], G_Loss[0.138], D_Real_Loss[0.737], D_Fake_Loss[0.553]
Progress[ 27%], ETA[  14m], Batch [ 830], G_Loss[0.091], D_Real_Loss[0.357], D_Fake_Loss[1.004]
Progress[ 28%], ETA[  14m], Batch [ 840], G_Loss[0.130], D_Real_Loss[0.625], D_Fake_Loss[0.579]
Progress[ 28%], ETA[  14m], Batch [ 850], G_Loss[0.180], D_Real_Loss[0.719], D_Fake_Loss[0.474]
Progress[ 28%], ETA[  14m], Batch [ 860], G_Loss[0.095], D_Real_Loss[0.552], D_Fake_Loss[0.828]
Progress[ 29%], ETA[  14m], Batch [ 870], G_Loss[0.119], D_Real_Loss[0.567], D_Fake_Loss[0.779]
Progress[ 29%], ETA[  14m], Batch [ 880], G_Loss[0.127], D_Real_Loss[0.594], D_Fake_Loss[0.588]
Progress[ 29%], ETA[  14m], Batch [ 890], G_Loss[0.134], D_Real_Loss[0.548], D_Fake_Loss[0.491]
Progress[ 30%], ETA[  13m], Batch [ 900], G_Loss[0.124], D_Real_Loss[0.763], D_Fake_Loss[0.638]
Progress[ 30%], ETA[  13m], Batch [ 910], G_Loss[0.133], D_Real_Loss[0.509], D_Fake_Loss[0.562]
Progress[ 30%], ETA[  13m], Batch [ 920], G_Loss[0.124], D_Real_Loss[0.551], D_Fake_Loss[0.643]
Progress[ 31%], ETA[  13m], Batch [ 930], G_Loss[0.105], D_Real_Loss[0.588], D_Fake_Loss[0.789]
Progress[ 31%], ETA[  13m], Batch [ 940], G_Loss[0.115], D_Real_Loss[0.735], D_Fake_Loss[0.737]
Progress[ 31%], ETA[  13m], Batch [ 950], G_Loss[0.140], D_Real_Loss[1.040], D_Fake_Loss[0.553]
Progress[ 32%], ETA[  13m], Batch [ 960], G_Loss[0.182], D_Real_Loss[1.000], D_Fake_Loss[0.294]
Progress[ 32%], ETA[  13m], Batch [ 970], G_Loss[0.149], D_Real_Loss[0.949], D_Fake_Loss[0.627]
Progress[ 32%], ETA[  13m], Batch [ 980], G_Loss[0.137], D_Real_Loss[0.569], D_Fake_Loss[0.568]
Progress[ 33%], ETA[  13m], Batch [ 990], G_Loss[0.143], D_Real_Loss[0.509], D_Fake_Loss[0.712]
Progress[ 33%], ETA[  13m], Batch [1000], G_Loss[0.135], D_Real_Loss[0.587], D_Fake_Loss[0.537]
    Saved train/batch001000_out.png
Progress[ 33%], ETA[  13m], Batch [1010], G_Loss[0.190], D_Real_Loss[1.385], D_Fake_Loss[0.277]
Progress[ 34%], ETA[  13m], Batch [1020], G_Loss[0.149], D_Real_Loss[0.617], D_Fake_Loss[0.606]
Progress[ 34%], ETA[  13m], Batch [1030], G_Loss[0.132], D_Real_Loss[0.723], D_Fake_Loss[0.546]
Progress[ 34%], ETA[  13m], Batch [1040], G_Loss[0.125], D_Real_Loss[0.604], D_Fake_Loss[0.550]
Progress[ 35%], ETA[  12m], Batch [1050], G_Loss[0.173], D_Real_Loss[1.256], D_Fake_Loss[0.301]
Progress[ 35%], ETA[  12m], Batch [1060], G_Loss[0.117], D_Real_Loss[0.514], D_Fake_Loss[0.649]
Progress[ 35%], ETA[  12m], Batch [1070], G_Loss[0.119], D_Real_Loss[0.508], D_Fake_Loss[0.650]
Progress[ 36%], ETA[  12m], Batch [1080], G_Loss[0.121], D_Real_Loss[0.599], D_Fake_Loss[0.766]
Progress[ 36%], ETA[  12m], Batch [1090], G_Loss[0.106], D_Real_Loss[0.451], D_Fake_Loss[0.705]
Progress[ 36%], ETA[  12m], Batch [1100], G_Loss[0.128], D_Real_Loss[0.602], D_Fake_Loss[0.679]
Progress[ 37%], ETA[  12m], Batch [1110], G_Loss[0.133], D_Real_Loss[1.025], D_Fake_Loss[0.563]
Progress[ 37%], ETA[  12m], Batch [1120], G_Loss[0.163], D_Real_Loss[0.716], D_Fake_Loss[0.349]
Progress[ 37%], ETA[  12m], Batch [1130], G_Loss[0.103], D_Real_Loss[0.434], D_Fake_Loss[0.769]
Progress[ 38%], ETA[  12m], Batch [1140], G_Loss[0.141], D_Real_Loss[0.973], D_Fake_Loss[0.464]
Progress[ 38%], ETA[  12m], Batch [1150], G_Loss[0.117], D_Real_Loss[0.489], D_Fake_Loss[0.662]
Progress[ 38%], ETA[  12m], Batch [1160], G_Loss[0.116], D_Real_Loss[0.480], D_Fake_Loss[0.698]
Progress[ 39%], ETA[  12m], Batch [1170], G_Loss[0.123], D_Real_Loss[0.642], D_Fake_Loss[0.603]
Progress[ 39%], ETA[  12m], Batch [1180], G_Loss[0.181], D_Real_Loss[1.119], D_Fake_Loss[0.316]
Progress[ 39%], ETA[  12m], Batch [1190], G_Loss[0.149], D_Real_Loss[0.592], D_Fake_Loss[0.469]
Progress[ 40%], ETA[  12m], Batch [1200], G_Loss[0.096], D_Real_Loss[0.386], D_Fake_Loss[0.962]
    Saved train/batch001200_out.png
Progress[ 40%], ETA[  11m], Batch [1210], G_Loss[0.129], D_Real_Loss[0.548], D_Fake_Loss[0.571]
Progress[ 40%], ETA[  11m], Batch [1220], G_Loss[0.138], D_Real_Loss[0.466], D_Fake_Loss[0.534]
Progress[ 41%], ETA[  11m], Batch [1230], G_Loss[0.131], D_Real_Loss[0.710], D_Fake_Loss[0.510]
Progress[ 41%], ETA[  11m], Batch [1240], G_Loss[0.141], D_Real_Loss[0.873], D_Fake_Loss[0.493]
Progress[ 41%], ETA[  11m], Batch [1250], G_Loss[0.127], D_Real_Loss[0.856], D_Fake_Loss[0.558]
Progress[ 42%], ETA[  11m], Batch [1260], G_Loss[0.116], D_Real_Loss[0.747], D_Fake_Loss[0.616]
Progress[ 42%], ETA[  11m], Batch [1270], G_Loss[0.148], D_Real_Loss[0.818], D_Fake_Loss[0.488]
Progress[ 42%], ETA[  11m], Batch [1280], G_Loss[0.100], D_Real_Loss[0.544], D_Fake_Loss[0.793]
Progress[ 43%], ETA[  11m], Batch [1290], G_Loss[0.106], D_Real_Loss[0.686], D_Fake_Loss[0.757]
Progress[ 43%], ETA[  11m], Batch [1300], G_Loss[0.105], D_Real_Loss[0.512], D_Fake_Loss[0.727]
Progress[ 43%], ETA[  11m], Batch [1310], G_Loss[0.109], D_Real_Loss[0.488], D_Fake_Loss[0.743]
Progress[ 44%], ETA[  11m], Batch [1320], G_Loss[0.188], D_Real_Loss[1.012], D_Fake_Loss[0.453]
Progress[ 44%], ETA[  11m], Batch [1330], G_Loss[0.090], D_Real_Loss[0.472], D_Fake_Loss[0.897]
Progress[ 44%], ETA[  11m], Batch [1340], G_Loss[0.117], D_Real_Loss[0.589], D_Fake_Loss[0.570]
Progress[ 45%], ETA[  11m], Batch [1350], G_Loss[0.124], D_Real_Loss[0.652], D_Fake_Loss[0.589]
Progress[ 45%], ETA[  10m], Batch [1360], G_Loss[0.110], D_Real_Loss[0.533], D_Fake_Loss[0.848]
Progress[ 45%], ETA[  10m], Batch [1370], G_Loss[0.126], D_Real_Loss[0.666], D_Fake_Loss[0.510]
Progress[ 45%], ETA[  10m], Batch [1380], G_Loss[0.095], D_Real_Loss[0.437], D_Fake_Loss[0.825]
Progress[ 46%], ETA[  10m], Batch [1390], G_Loss[0.105], D_Real_Loss[0.563], D_Fake_Loss[0.836]
Progress[ 46%], ETA[  10m], Batch [1400], G_Loss[0.104], D_Real_Loss[0.601], D_Fake_Loss[0.775]
    Saved train/batch001400_out.png
Progress[ 47%], ETA[  10m], Batch [1410], G_Loss[0.117], D_Real_Loss[0.649], D_Fake_Loss[0.595]
Progress[ 47%], ETA[  10m], Batch [1420], G_Loss[0.169], D_Real_Loss[0.811], D_Fake_Loss[0.406]
Progress[ 47%], ETA[  10m], Batch [1430], G_Loss[0.122], D_Real_Loss[0.623], D_Fake_Loss[0.662]
Progress[ 48%], ETA[  10m], Batch [1440], G_Loss[0.093], D_Real_Loss[0.398], D_Fake_Loss[0.869]
Progress[ 48%], ETA[  10m], Batch [1450], G_Loss[0.112], D_Real_Loss[0.593], D_Fake_Loss[0.698]
Progress[ 48%], ETA[  10m], Batch [1460], G_Loss[0.102], D_Real_Loss[0.540], D_Fake_Loss[0.795]
Progress[ 49%], ETA[  10m], Batch [1470], G_Loss[0.159], D_Real_Loss[0.951], D_Fake_Loss[0.380]
Progress[ 49%], ETA[  10m], Batch [1480], G_Loss[0.127], D_Real_Loss[0.579], D_Fake_Loss[0.532]
Progress[ 49%], ETA[  10m], Batch [1490], G_Loss[0.092], D_Real_Loss[0.539], D_Fake_Loss[0.871]
Progress[ 50%], ETA[  10m], Batch [1500], G_Loss[0.138], D_Real_Loss[0.860], D_Fake_Loss[0.470]
Progress[ 50%], ETA[   9m], Batch [1510], G_Loss[0.122], D_Real_Loss[0.618], D_Fake_Loss[0.630]
Progress[ 50%], ETA[   9m], Batch [1520], G_Loss[0.100], D_Real_Loss[0.449], D_Fake_Loss[0.832]
Progress[ 50%], ETA[   9m], Batch [1530], G_Loss[0.106], D_Real_Loss[0.373], D_Fake_Loss[0.696]
Progress[ 51%], ETA[   9m], Batch [1540], G_Loss[0.094], D_Real_Loss[0.533], D_Fake_Loss[0.898]
Progress[ 51%], ETA[   9m], Batch [1550], G_Loss[0.083], D_Real_Loss[0.496], D_Fake_Loss[0.976]
Progress[ 52%], ETA[   9m], Batch [1560], G_Loss[0.133], D_Real_Loss[0.871], D_Fake_Loss[0.483]
Progress[ 52%], ETA[   9m], Batch [1570], G_Loss[0.112], D_Real_Loss[0.610], D_Fake_Loss[0.646]
Progress[ 52%], ETA[   9m], Batch [1580], G_Loss[0.130], D_Real_Loss[0.729], D_Fake_Loss[0.539]
Progress[ 53%], ETA[   9m], Batch [1590], G_Loss[0.116], D_Real_Loss[0.502], D_Fake_Loss[0.621]
Progress[ 53%], ETA[   9m], Batch [1600], G_Loss[0.136], D_Real_Loss[0.764], D_Fake_Loss[0.496]
    Saved train/batch001600_out.png
Progress[ 53%], ETA[   9m], Batch [1610], G_Loss[0.092], D_Real_Loss[0.483], D_Fake_Loss[0.887]
Progress[ 54%], ETA[   9m], Batch [1620], G_Loss[0.127], D_Real_Loss[0.642], D_Fake_Loss[0.564]
Progress[ 54%], ETA[   9m], Batch [1630], G_Loss[0.124], D_Real_Loss[0.681], D_Fake_Loss[0.571]
Progress[ 54%], ETA[   9m], Batch [1640], G_Loss[0.124], D_Real_Loss[0.657], D_Fake_Loss[0.534]
Progress[ 55%], ETA[   9m], Batch [1650], G_Loss[0.094], D_Real_Loss[0.542], D_Fake_Loss[0.768]
Progress[ 55%], ETA[   8m], Batch [1660], G_Loss[0.119], D_Real_Loss[0.582], D_Fake_Loss[0.600]
Progress[ 55%], ETA[   8m], Batch [1670], G_Loss[0.104], D_Real_Loss[0.570], D_Fake_Loss[0.865]
Progress[ 56%], ETA[   8m], Batch [1680], G_Loss[0.091], D_Real_Loss[0.421], D_Fake_Loss[0.896]
Progress[ 56%], ETA[   8m], Batch [1690], G_Loss[0.131], D_Real_Loss[0.704], D_Fake_Loss[0.509]
Progress[ 56%], ETA[   8m], Batch [1700], G_Loss[0.113], D_Real_Loss[0.615], D_Fake_Loss[0.722]
Progress[ 57%], ETA[   8m], Batch [1710], G_Loss[0.144], D_Real_Loss[0.722], D_Fake_Loss[0.637]
Progress[ 57%], ETA[   8m], Batch [1720], G_Loss[0.132], D_Real_Loss[0.612], D_Fake_Loss[0.607]
Progress[ 57%], ETA[   8m], Batch [1730], G_Loss[0.130], D_Real_Loss[0.518], D_Fake_Loss[0.629]
Progress[ 58%], ETA[   8m], Batch [1740], G_Loss[0.133], D_Real_Loss[0.759], D_Fake_Loss[0.513]
Progress[ 58%], ETA[   8m], Batch [1750], G_Loss[0.098], D_Real_Loss[0.684], D_Fake_Loss[0.850]
Progress[ 58%], ETA[   8m], Batch [1760], G_Loss[0.133], D_Real_Loss[0.841], D_Fake_Loss[0.492]
Progress[ 58%], ETA[   8m], Batch [1770], G_Loss[0.136], D_Real_Loss[0.612], D_Fake_Loss[0.481]
Progress[ 59%], ETA[   8m], Batch [1780], G_Loss[0.133], D_Real_Loss[0.705], D_Fake_Loss[0.506]
Progress[ 59%], ETA[   8m], Batch [1790], G_Loss[0.110], D_Real_Loss[0.763], D_Fake_Loss[0.668]
Progress[ 59%], ETA[   8m], Batch [1800], G_Loss[0.131], D_Real_Loss[0.775], D_Fake_Loss[0.548]
    Saved train/batch001800_out.png
Progress[ 60%], ETA[   7m], Batch [1810], G_Loss[0.120], D_Real_Loss[0.617], D_Fake_Loss[0.563]
Progress[ 60%], ETA[   7m], Batch [1820], G_Loss[0.154], D_Real_Loss[0.930], D_Fake_Loss[0.419]
Progress[ 61%], ETA[   7m], Batch [1830], G_Loss[0.112], D_Real_Loss[0.666], D_Fake_Loss[0.654]
Progress[ 61%], ETA[   7m], Batch [1840], G_Loss[0.140], D_Real_Loss[0.750], D_Fake_Loss[0.441]
Progress[ 61%], ETA[   7m], Batch [1850], G_Loss[0.118], D_Real_Loss[0.577], D_Fake_Loss[0.692]
Progress[ 62%], ETA[   7m], Batch [1860], G_Loss[0.095], D_Real_Loss[0.366], D_Fake_Loss[0.822]
Progress[ 62%], ETA[   7m], Batch [1870], G_Loss[0.131], D_Real_Loss[0.600], D_Fake_Loss[0.637]
Progress[ 62%], ETA[   7m], Batch [1880], G_Loss[0.141], D_Real_Loss[0.807], D_Fake_Loss[0.538]
Progress[ 63%], ETA[   7m], Batch [1890], G_Loss[0.098], D_Real_Loss[0.668], D_Fake_Loss[0.754]
Progress[ 63%], ETA[   7m], Batch [1900], G_Loss[0.126], D_Real_Loss[0.681], D_Fake_Loss[0.522]
Progress[ 63%], ETA[   7m], Batch [1910], G_Loss[0.118], D_Real_Loss[0.428], D_Fake_Loss[0.629]
Progress[ 63%], ETA[   7m], Batch [1920], G_Loss[0.061], D_Real_Loss[0.248], D_Fake_Loss[1.475]
Progress[ 64%], ETA[   7m], Batch [1930], G_Loss[0.121], D_Real_Loss[0.485], D_Fake_Loss[0.699]
Progress[ 64%], ETA[   7m], Batch [1940], G_Loss[0.087], D_Real_Loss[0.366], D_Fake_Loss[0.930]
Progress[ 64%], ETA[   7m], Batch [1950], G_Loss[0.124], D_Real_Loss[0.482], D_Fake_Loss[0.634]
Progress[ 65%], ETA[   6m], Batch [1960], G_Loss[0.145], D_Real_Loss[0.578], D_Fake_Loss[0.489]
Progress[ 65%], ETA[   6m], Batch [1970], G_Loss[0.113], D_Real_Loss[0.458], D_Fake_Loss[0.721]
Progress[ 65%], ETA[   6m], Batch [1980], G_Loss[0.138], D_Real_Loss[0.542], D_Fake_Loss[0.481]
Progress[ 66%], ETA[   6m], Batch [1990], G_Loss[0.119], D_Real_Loss[0.727], D_Fake_Loss[0.592]
Progress[ 66%], ETA[   6m], Batch [2000], G_Loss[0.152], D_Real_Loss[0.821], D_Fake_Loss[0.451]
    Saved train/batch002000_out.png
Progress[ 67%], ETA[   6m], Batch [2010], G_Loss[0.101], D_Real_Loss[0.452], D_Fake_Loss[0.870]
Progress[ 67%], ETA[   6m], Batch [2020], G_Loss[0.145], D_Real_Loss[0.792], D_Fake_Loss[0.483]
Progress[ 67%], ETA[   6m], Batch [2030], G_Loss[0.124], D_Real_Loss[0.558], D_Fake_Loss[0.882]
Progress[ 68%], ETA[   6m], Batch [2040], G_Loss[0.096], D_Real_Loss[0.547], D_Fake_Loss[0.925]
Progress[ 68%], ETA[   6m], Batch [2050], G_Loss[0.104], D_Real_Loss[0.525], D_Fake_Loss[0.819]
Progress[ 68%], ETA[   6m], Batch [2060], G_Loss[0.146], D_Real_Loss[0.796], D_Fake_Loss[0.420]
Progress[ 68%], ETA[   6m], Batch [2070], G_Loss[0.103], D_Real_Loss[0.524], D_Fake_Loss[0.766]
Progress[ 69%], ETA[   6m], Batch [2080], G_Loss[0.122], D_Real_Loss[0.640], D_Fake_Loss[0.702]
Progress[ 69%], ETA[   6m], Batch [2090], G_Loss[0.112], D_Real_Loss[0.627], D_Fake_Loss[0.731]
Progress[ 69%], ETA[   6m], Batch [2100], G_Loss[0.094], D_Real_Loss[0.431], D_Fake_Loss[0.919]
Progress[ 70%], ETA[   5m], Batch [2110], G_Loss[0.122], D_Real_Loss[0.425], D_Fake_Loss[0.757]
Progress[ 70%], ETA[   5m], Batch [2120], G_Loss[0.094], D_Real_Loss[0.367], D_Fake_Loss[0.888]
Progress[ 70%], ETA[   5m], Batch [2130], G_Loss[0.104], D_Real_Loss[0.622], D_Fake_Loss[0.728]
Progress[ 71%], ETA[   5m], Batch [2140], G_Loss[0.100], D_Real_Loss[0.467], D_Fake_Loss[0.895]
Progress[ 71%], ETA[   5m], Batch [2150], G_Loss[0.074], D_Real_Loss[0.215], D_Fake_Loss[1.345]
Progress[ 71%], ETA[   5m], Batch [2160], G_Loss[0.103], D_Real_Loss[0.511], D_Fake_Loss[0.748]
Progress[ 72%], ETA[   5m], Batch [2170], G_Loss[0.129], D_Real_Loss[0.613], D_Fake_Loss[0.537]
Progress[ 72%], ETA[   5m], Batch [2180], G_Loss[0.111], D_Real_Loss[0.707], D_Fake_Loss[0.646]
Progress[ 72%], ETA[   5m], Batch [2190], G_Loss[0.140], D_Real_Loss[0.847], D_Fake_Loss[0.514]
Progress[ 73%], ETA[   5m], Batch [2200], G_Loss[0.166], D_Real_Loss[0.585], D_Fake_Loss[0.512]
    Saved train/batch002200_out.png
Progress[ 73%], ETA[   5m], Batch [2210], G_Loss[0.207], D_Real_Loss[1.108], D_Fake_Loss[0.216]
Progress[ 73%], ETA[   5m], Batch [2220], G_Loss[0.089], D_Real_Loss[0.341], D_Fake_Loss[0.973]
Progress[ 74%], ETA[   5m], Batch [2230], G_Loss[0.102], D_Real_Loss[0.390], D_Fake_Loss[1.097]
Progress[ 74%], ETA[   5m], Batch [2240], G_Loss[0.129], D_Real_Loss[0.529], D_Fake_Loss[0.549]
Progress[ 74%], ETA[   5m], Batch [2250], G_Loss[0.115], D_Real_Loss[0.466], D_Fake_Loss[0.835]
Progress[ 75%], ETA[   4m], Batch [2260], G_Loss[0.097], D_Real_Loss[0.551], D_Fake_Loss[0.794]
Progress[ 75%], ETA[   4m], Batch [2270], G_Loss[0.064], D_Real_Loss[0.345], D_Fake_Loss[1.491]
Progress[ 75%], ETA[   4m], Batch [2280], G_Loss[0.092], D_Real_Loss[0.357], D_Fake_Loss[0.870]
Progress[ 76%], ETA[   4m], Batch [2290], G_Loss[0.123], D_Real_Loss[0.483], D_Fake_Loss[0.636]
Progress[ 76%], ETA[   4m], Batch [2300], G_Loss[0.099], D_Real_Loss[0.475], D_Fake_Loss[0.833]
Progress[ 76%], ETA[   4m], Batch [2310], G_Loss[0.124], D_Real_Loss[0.572], D_Fake_Loss[0.548]
Progress[ 77%], ETA[   4m], Batch [2320], G_Loss[0.177], D_Real_Loss[0.941], D_Fake_Loss[0.309]
Progress[ 77%], ETA[   4m], Batch [2330], G_Loss[0.116], D_Real_Loss[0.673], D_Fake_Loss[0.617]
Progress[ 77%], ETA[   4m], Batch [2340], G_Loss[0.149], D_Real_Loss[0.658], D_Fake_Loss[0.405]
Progress[ 78%], ETA[   4m], Batch [2350], G_Loss[0.123], D_Real_Loss[0.564], D_Fake_Loss[0.562]
Progress[ 78%], ETA[   4m], Batch [2360], G_Loss[0.136], D_Real_Loss[0.621], D_Fake_Loss[0.491]
Progress[ 78%], ETA[   4m], Batch [2370], G_Loss[0.079], D_Real_Loss[0.363], D_Fake_Loss[1.043]
Progress[ 79%], ETA[   4m], Batch [2380], G_Loss[0.134], D_Real_Loss[0.638], D_Fake_Loss[0.552]
Progress[ 79%], ETA[   4m], Batch [2390], G_Loss[0.126], D_Real_Loss[0.960], D_Fake_Loss[0.547]
Progress[ 79%], ETA[   4m], Batch [2400], G_Loss[0.143], D_Real_Loss[0.930], D_Fake_Loss[0.485]
    Saved train/batch002400_out.png
Progress[ 80%], ETA[   3m], Batch [2410], G_Loss[0.103], D_Real_Loss[0.474], D_Fake_Loss[0.718]
Progress[ 80%], ETA[   3m], Batch [2420], G_Loss[0.115], D_Real_Loss[0.670], D_Fake_Loss[0.627]
Progress[ 80%], ETA[   3m], Batch [2430], G_Loss[0.167], D_Real_Loss[0.819], D_Fake_Loss[0.350]
Progress[ 81%], ETA[   3m], Batch [2440], G_Loss[0.137], D_Real_Loss[0.738], D_Fake_Loss[0.494]
Progress[ 81%], ETA[   3m], Batch [2450], G_Loss[0.097], D_Real_Loss[0.572], D_Fake_Loss[0.879]
Progress[ 81%], ETA[   3m], Batch [2460], G_Loss[0.161], D_Real_Loss[0.581], D_Fake_Loss[0.376]
Progress[ 82%], ETA[   3m], Batch [2470], G_Loss[0.094], D_Real_Loss[0.468], D_Fake_Loss[0.911]
Progress[ 82%], ETA[   3m], Batch [2480], G_Loss[0.132], D_Real_Loss[0.658], D_Fake_Loss[0.509]
Progress[ 82%], ETA[   3m], Batch [2490], G_Loss[0.129], D_Real_Loss[0.732], D_Fake_Loss[0.546]
Progress[ 83%], ETA[   3m], Batch [2500], G_Loss[0.129], D_Real_Loss[0.587], D_Fake_Loss[0.574]
Progress[ 83%], ETA[   3m], Batch [2510], G_Loss[0.117], D_Real_Loss[0.546], D_Fake_Loss[0.644]
Progress[ 83%], ETA[   3m], Batch [2520], G_Loss[0.083], D_Real_Loss[0.322], D_Fake_Loss[0.923]
Progress[ 84%], ETA[   3m], Batch [2530], G_Loss[0.104], D_Real_Loss[0.561], D_Fake_Loss[0.780]
Progress[ 84%], ETA[   3m], Batch [2540], G_Loss[0.143], D_Real_Loss[0.782], D_Fake_Loss[0.464]
Progress[ 84%], ETA[   3m], Batch [2550], G_Loss[0.151], D_Real_Loss[0.738], D_Fake_Loss[0.394]
Progress[ 85%], ETA[   2m], Batch [2560], G_Loss[0.110], D_Real_Loss[0.365], D_Fake_Loss[0.728]
Progress[ 85%], ETA[   2m], Batch [2570], G_Loss[0.137], D_Real_Loss[0.866], D_Fake_Loss[0.481]
Progress[ 85%], ETA[   2m], Batch [2580], G_Loss[0.133], D_Real_Loss[0.876], D_Fake_Loss[0.512]
Progress[ 86%], ETA[   2m], Batch [2590], G_Loss[0.126], D_Real_Loss[0.756], D_Fake_Loss[0.520]
Progress[ 86%], ETA[   2m], Batch [2600], G_Loss[0.110], D_Real_Loss[0.637], D_Fake_Loss[0.676]
    Saved train/batch002600_out.png
Progress[ 86%], ETA[   2m], Batch [2610], G_Loss[0.113], D_Real_Loss[0.265], D_Fake_Loss[1.080]
Progress[ 87%], ETA[   2m], Batch [2620], G_Loss[0.110], D_Real_Loss[0.563], D_Fake_Loss[0.693]
Progress[ 87%], ETA[   2m], Batch [2630], G_Loss[0.094], D_Real_Loss[0.414], D_Fake_Loss[0.869]
Progress[ 87%], ETA[   2m], Batch [2640], G_Loss[0.127], D_Real_Loss[0.431], D_Fake_Loss[0.551]
Progress[ 88%], ETA[   2m], Batch [2650], G_Loss[0.128], D_Real_Loss[0.473], D_Fake_Loss[0.770]
Progress[ 88%], ETA[   2m], Batch [2660], G_Loss[0.108], D_Real_Loss[0.635], D_Fake_Loss[0.791]
Progress[ 88%], ETA[   2m], Batch [2670], G_Loss[0.202], D_Real_Loss[1.019], D_Fake_Loss[0.244]
Progress[ 89%], ETA[   2m], Batch [2680], G_Loss[0.137], D_Real_Loss[0.610], D_Fake_Loss[0.760]
Progress[ 89%], ETA[   2m], Batch [2690], G_Loss[0.125], D_Real_Loss[0.528], D_Fake_Loss[0.565]
Progress[ 89%], ETA[   2m], Batch [2700], G_Loss[0.100], D_Real_Loss[0.406], D_Fake_Loss[0.800]
Progress[ 90%], ETA[   1m], Batch [2710], G_Loss[0.149], D_Real_Loss[0.656], D_Fake_Loss[0.414]
Progress[ 90%], ETA[   1m], Batch [2720], G_Loss[0.120], D_Real_Loss[0.684], D_Fake_Loss[0.601]
Progress[ 90%], ETA[   1m], Batch [2730], G_Loss[0.221], D_Real_Loss[1.306], D_Fake_Loss[0.186]
Progress[ 91%], ETA[   1m], Batch [2740], G_Loss[0.098], D_Real_Loss[0.408], D_Fake_Loss[0.812]
Progress[ 91%], ETA[   1m], Batch [2750], G_Loss[0.154], D_Real_Loss[0.934], D_Fake_Loss[0.377]
Progress[ 91%], ETA[   1m], Batch [2760], G_Loss[0.157], D_Real_Loss[1.084], D_Fake_Loss[0.409]
Progress[ 92%], ETA[   1m], Batch [2770], G_Loss[0.129], D_Real_Loss[0.718], D_Fake_Loss[0.660]
Progress[ 92%], ETA[   1m], Batch [2780], G_Loss[0.136], D_Real_Loss[0.811], D_Fake_Loss[0.499]
Progress[ 92%], ETA[   1m], Batch [2790], G_Loss[0.204], D_Real_Loss[0.681], D_Fake_Loss[0.272]
Progress[ 93%], ETA[   1m], Batch [2800], G_Loss[0.120], D_Real_Loss[0.369], D_Fake_Loss[0.742]
    Saved train/batch002800_out.png
Progress[ 93%], ETA[   1m], Batch [2810], G_Loss[0.137], D_Real_Loss[0.422], D_Fake_Loss[0.550]
Progress[ 93%], ETA[   1m], Batch [2820], G_Loss[0.173], D_Real_Loss[0.827], D_Fake_Loss[0.412]
Progress[ 94%], ETA[   1m], Batch [2830], G_Loss[0.123], D_Real_Loss[0.772], D_Fake_Loss[0.632]
Progress[ 94%], ETA[   1m], Batch [2840], G_Loss[0.119], D_Real_Loss[0.725], D_Fake_Loss[0.603]
Progress[ 94%], ETA[   1m], Batch [2850], G_Loss[0.167], D_Real_Loss[0.967], D_Fake_Loss[0.328]
Progress[ 95%], ETA[   0m], Batch [2860], G_Loss[0.143], D_Real_Loss[0.496], D_Fake_Loss[0.474]
Progress[ 95%], ETA[   0m], Batch [2870], G_Loss[0.144], D_Real_Loss[0.503], D_Fake_Loss[0.495]
Progress[ 95%], ETA[   0m], Batch [2880], G_Loss[0.146], D_Real_Loss[0.716], D_Fake_Loss[0.431]
Progress[ 96%], ETA[   0m], Batch [2890], G_Loss[0.183], D_Real_Loss[0.925], D_Fake_Loss[0.336]
Progress[ 96%], ETA[   0m], Batch [2900], G_Loss[0.062], D_Real_Loss[0.316], D_Fake_Loss[1.581]
Progress[ 96%], ETA[   0m], Batch [2910], G_Loss[0.156], D_Real_Loss[0.735], D_Fake_Loss[0.405]
Progress[ 97%], ETA[   0m], Batch [2920], G_Loss[0.209], D_Real_Loss[0.722], D_Fake_Loss[0.227]
Progress[ 97%], ETA[   0m], Batch [2930], G_Loss[0.100], D_Real_Loss[0.357], D_Fake_Loss[0.780]
Progress[ 97%], ETA[   0m], Batch [2940], G_Loss[0.165], D_Real_Loss[0.465], D_Fake_Loss[0.349]
Progress[ 98%], ETA[   0m], Batch [2950], G_Loss[0.208], D_Real_Loss[0.641], D_Fake_Loss[0.296]
Progress[ 98%], ETA[   0m], Batch [2960], G_Loss[0.128], D_Real_Loss[0.577], D_Fake_Loss[0.533]
Progress[ 98%], ETA[   0m], Batch [2970], G_Loss[0.159], D_Real_Loss[0.989], D_Fake_Loss[0.363]
Progress[ 99%], ETA[   0m], Batch [2980], G_Loss[0.170], D_Real_Loss[0.668], D_Fake_Loss[0.386]
Progress[ 99%], ETA[   0m], Batch [2990], G_Loss[0.097], D_Real_Loss[0.474], D_Fake_Loss[0.899]
Progress[ 99%], ETA[   0m], Batch [3000], G_Loss[0.120], D_Real_Loss[0.683], D_Fake_Loss[0.672]
    Saved train/batch003000_out.png
Progress[100%], ETA[   0m], Batch [3010], G_Loss[0.215], D_Real_Loss[0.702], D_Fake_Loss[0.197]
    Checkpoint saved
Finished training!
